# 200309 (월) 수업

1. 퍼셉트론(perceptron)

   : 입출력을 갖춘 알고리즘

   : 다수의 신호를 입력으로 받아 하나의 신호를 출력한다. 

   : '가중치'와 '편향'을 매개변수로 설정한다.

   `가중치`, `임계값`,  `입력 값`,  `출력 값`

   

   * 단층 퍼셉트론(single lyer perceptron)

   <img src="https://t1.daumcdn.net/cfile/tistory/23573D3656C2D8421C" alt="img" style="zoom: 50%;" />

   

   

   * 다층 퍼셉트론(multilayer perceptron: MLP)

   ​       : XOR 게이트를 구현 가능, 단층 퍼셉트론으로는 표현하지 못한 것을 층을 늘려 구현 가능

   <img src="https://www.researchgate.net/profile/Qianzhou_Du2/publication/322477802/figure/fig1/AS:582461357019136@1515881017619/Multiple-Layer-Perceptron-in-Neural-Networks.png" alt="Multiple Layer Perceptron in Neural Networks. " style="zoom: 67%;" />

   

   > 일반적으로 단순 퍼셉트론은 **단층 네트워크**에서 계단 함수(임계값을 경계로 출력이 바뀌는 함수)를 활성화 함수로 사용한 모델을 가리키고 **다층 퍼셉트론**은 신경망(여러 층으로 구성되고 시그모이드 함수 등의 매끈한 활성화 함수를 사용하는 네트워크)을 가리킨다.

   

2. 역전파 학습 알고리즘(Backpropagation)

   

3. 활성화 함수(Activation Function)

   : 입력 신호의 총합을 출력 신호로 변환하는 함수

   : 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할

   * 계단 함수(step fucntion)

   * 시그모이드함수(sigmoid function)

   * Tanh Function

   * ReLU Function (Rectified Linear Unit)

     

4. 손실 함수(Loss Function)

   

5. 경사하강법(Gradient descent)

   : 학습 단계에서 최적의 매개변수(가중치와 편향)을 찾을 때, (여기서 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값이다.) 손실 함수는 매우 복잡하므로 이런 상황에서 기울기를 잘 이용해 함수의 최솟값 (또는 가능한 작은 값)을 찾으려는 것이 경사법이다.

   >함수가 극솟값, 최솟값, 또 안장점(saddle point)이 되는 장소에서는 기울기가 0이다. 경사법은 기울기가 0인 장소를 찾지만 그것이 반드시 최솟값이라고 할 수 없다. (극솟값이나 안장점일 가능성 존재) 또, 복잡하고 찌그러진 모양의 함수라면(대부분) 평평한 곳으로 파고들면서 고원(plateau)이라 하는, 학습이 진행되지 않는 정체기에 빠질 수 있다. 

   

6. ML & Deep Learning Framework

   * Tensorflow

   * Pytorch

   * Tensorflow + Keras

     

7. Tensorflow 설치 및 예제

- Tensorflow 구조

  <img src="https://miro.medium.com/max/517/0*4yvH8k8HfkbHAT4u.png" alt="img" style="zoom: 80%;" />

  

- Anaconda Prompt

  ```powershell
  cd c:\py_data
  conda inf --envs # 가상환경 list
  conda create -m py36 python=3.6 tensorflow==1.6.0
  # Anaconda : PackagesNotFoundError: The following packages are not available from current channels 에러
  
  # 내가 한 방법
  cd c:\py_data 
  conda create --name py36 python=3.6
  conda activate py36
  pip install tensorflow==1.6.0
  conda install jupyter
  jupyter notebook # 실행하면 chrome 창 open
  
  ```

  

- 경사 하강법 프로그래밍

  ```python
  # jupyter notebook [py_data]의 deep learning 에 작성
  
  x = 10  
  learning_rate = 0.01  
  precision = 0.00001  
  max_iterationsterations = 100
  
  # 손실 함수를 람다식으로 정의한다. 
  loss_func = lambda x: (x-3)**2 + 10
  
  # 그래디언트를 람다식으로 정의한다. 손실 함수의 1차 미분값이다. 
  gradient = lambda x: 2*x-6
  
  # 그래디언트 강하법
  for i in range(max_iterations):
      x = x - learning_rate * gradient(x)
      print("손실 함수값(", x, ")=", loss_func(x))
  
  print("최소값 = ", x)
  
  ```

  